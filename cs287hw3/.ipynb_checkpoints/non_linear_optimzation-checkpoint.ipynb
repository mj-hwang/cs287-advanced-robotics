{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import copy\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import *\n",
    "from envs.cart_pole_env import CartPoleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Linear Environment\n",
    "We start with the linear environment, similar to the one on the previous homework, and we will consider optimizing for a sequence of actions, comparing shooting and collocation.\n",
    "\n",
    "First, we define the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEnv(object):\n",
    "    def __init__(self, horizon=20, multiplier=1.):\n",
    "        self.A = multiplier * 0.1 * np.array([[0.0481, -0.5049, 0.0299, 2.6544, 1.0608],\n",
    "                                 [2.3846, -0.2312, -0.1260, -0.7945, 0.5279],\n",
    "                                 [1.4019, -0.6394, -0.1401, 0.5484, 0.1624],\n",
    "                                 [-0.0254, 0.4595, -0.0862, 2.1750, 1.1012],\n",
    "                                 [0.5172, 0.5060, 1.6579, -0.9407, -1.4441]])\n",
    "        self.B = np.array([[-0.7789, -1.2076],\n",
    "                           [0.4299, -1.6041],\n",
    "                           [0.2006, -1.7395],\n",
    "                           [0.8302, 0.2295],\n",
    "                           [-1.8465, 1.2780]])\n",
    "        self.H = 20\n",
    "\n",
    "        self.dx = self.A.shape[1]\n",
    "        self.du = self.B.shape[1]\n",
    "        self.Q = np.eye(self.dx)\n",
    "        self.R = np.eye(self.du)\n",
    "        self._init_state =  np.array([-1.9613, -1.3127, 0.0698, 0.0935, 1.2494])\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, act):\n",
    "        cost = self._state.T @ self.Q @ self._state + act.T @ self.R @ act\n",
    "        state = self.A @ self._state + self.B @ act\n",
    "        self._state = state.copy()\n",
    "        return state, cost, False, {}\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self._state = state.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = self._init_state.copy()\n",
    "        return self._init_state.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LinearEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the non-linear optimzation algorithms. A correct implementation for both methods should \n",
    "give an optimal cost of 7.461 for both methods, and a collocation error of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Shooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the shooting method, we look for the sequences of actions that minimize the sequences the total cost by directly substuting the constraints in the objective:\n",
    "\n",
    "$$ \\min_{u_0, \\dots, u_H} c(x_0, u_0) + c(f(x_0, u_0), u_1) + c(f(f(x_0, u_0), u_1) \\cdots $$\n",
    "\n",
    "\n",
    "In order to perform the optimization, we need to define the objective function to optimize.\n",
    "Fill in the code in ```eval_shooting``` which should return the cost of the trajectory\n",
    "with the specified sequences of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_shooting(env, actions):\n",
    "    \"\"\"\n",
    "    Find the cumulative cost of the sequences of actions, which has shape [horizon, action dimension].\n",
    "    Use the function step of the environment: env.step(action). It returns: next_observation, cost, done,\n",
    "    env_infos.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    actions = actions.reshape(env.H, env.du)\n",
    "    horizon = env.H\n",
    "    \n",
    "    total_cost = 0\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the objective function, we can use an off-the-shelf optimizer \n",
    "to find the optimal actions. In these case, we use \n",
    "[BFGS](https://docs.scipy.org/doc/scipy-0.16.0/reference/optimize.minimize-bfgs.html#optimize-minimize-bfgs),\n",
    "which is a quasi-Newton method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "The optimal cost is 7.461\n"
     ]
    }
   ],
   "source": [
    "def minimize_shooting(env):\n",
    "    \"\"\"\n",
    "    Finds the sequences of actions that minmize the cost specified by the environment env.\n",
    "    \"\"\"\n",
    "    init_actions = np.random.uniform(low=-.1, high=.1, size=(env.H * env.du,))\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    res = minimize(fun=# Fill this with a function that returns the cumulative cost given the states and actions,\n",
    "               x0=# Fill this with the inital actions, \n",
    "               method='BFGS',\n",
    "               options={'xtol': 1e-6, 'disp': False, 'verbose': 2}\n",
    "              )\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    print(res.message)\n",
    "    print(\"The optimal cost is %.3f\" % res.fun)\n",
    "    policy_shooting = ActPolicy(env=env, \n",
    "                                actions=res.x,\n",
    "                               )\n",
    "    return policy_shooting\n",
    "\n",
    "policy_shooting = minimize_shooting(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Collocation\n",
    "Now we will do the same, but for the collocation method.  In addition to the objective function, we also have to formulate the equality constraints that capture the dynamics.\n",
    "\n",
    "$$ \\min_{u_0, x_1, u_1, \\dots, x_H, u_H} c(x_0, u_0) + c(x_1, u_1) + \\cdots + c(x_H, u_H)$$\n",
    "\n",
    "Fill in the code in ``eval_collocation`` and ``constraints``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collocation(env, x):\n",
    "    \"\"\"\n",
    "    Find the cost of the sequences of actions and state that have shape [horizon, action dimension]\n",
    "    and [horizon, state_dim], respectively.\n",
    "    Use the function step of the environment: env.step(action). It returns, next_observation, cost, done,\n",
    "    env_infos.\n",
    "    In order to set the environment at a specific state use the function env.set_state(state)\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_cost = 0\n",
    "    states, actions = x[:env.H * env.dx], x[env.H * env.dx:]\n",
    "    states = states.reshape(env.H, env.dx)\n",
    "    actions = actions.reshape(env.H, env.du)\n",
    "    horizon = env.H\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "   \n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return total_cost\n",
    "\n",
    "def constraints(env, x):\n",
    "    \"\"\"\n",
    "    Create a list in which each element is constraint on the dynamics that should be equal to 0.\n",
    "    Use the function step of the environment: env.step(action). It returns, next_observation, cost, done,\n",
    "    env_infos.\n",
    "    \n",
    "    For each time step, you need to fill the list constraints with the error in the state \n",
    "    variable and the actial state. This means state_{t+1} - f(state_t, action_t), for t = 0, ..., horizon-1.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    constraints = []\n",
    "    states, actions = x[:env.H * env.dx], x[env.H * env.dx:]\n",
    "    states = states.reshape(env.H, env.dx)\n",
    "    actions = actions.reshape(env.H, env.du)\n",
    "    horizon = env.H\n",
    "    \n",
    "\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return np.concatenate(constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can too use an off-the-shelf constraint optimzation algortihm, in thise case, we make use of the\n",
    "[SLQP](https://docs.scipy.org/doc/scipy-0.16.0/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp)\n",
    " algorithm, which was seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "The optimal cost is 7.461\n"
     ]
    }
   ],
   "source": [
    "def minimize_collocation(env):\n",
    "    init_states_and_actions = np.random.uniform(low=-.1, high=.1, size=(env.H * (env.du + env.dx),))\n",
    "\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    eq_cons = {'type': 'eq',\n",
    "               'fun' : # Fill this with a function that returns the constraints given the states and actions              }\n",
    "\n",
    "    res = minimize(fun=# Fill this with a function that returns the cumulative cost given the states and actions,\n",
    "                   x0= # Fill this with the initial actions,\n",
    "                   method='SLSQP', \n",
    "                   constraints=eq_cons,\n",
    "                   options={'xtol': 1e-6, 'disp': False, 'verbose': 0, 'maxiter':201}\n",
    "                  )\n",
    "    print(res.message)\n",
    "    print(\"The optimal cost is %.3f\" % res.fun)\n",
    "    states_collocation, act_collocation = res.x[:env.H * env.dx], res.x[env.H * env.dx:]\n",
    "    states_collocation = states_collocation.reshape(env.H, env.dx)\n",
    "    policy_collocation = ActPolicy(env,\n",
    "                                   actions=act_collocation\n",
    "                                  )\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return policy_collocation, states_collocation\n",
    "\n",
    "policy_collocation, states_collocation = minimize_collocation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Quantitative Metrics ---\n",
      "Shooting Cost 7.461\n",
      "Collocation Cost 7.461\n",
      "Collocation Error 0.000\n",
      "\n",
      "\n",
      "---- Qualitative Metrics ---\n",
      "Evolution of the value of each dimension across 20 timesteps for the shooting methods.\n",
      "Both methods converge to the origin. Shooting: solid line(-);  Collocation: dashed line(--).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_shoot, states_shoot = rollout(env, policy_shooting)\n",
    "cost_col, states_col = rollout(env, policy_collocation)\n",
    "states_shoot, states_col = np.array(states_shoot), np.array(states_col)\n",
    "error = np.linalg.norm(states_col[1:,:] - np.array(states_collocation))\n",
    "ts = np.arange(states_shoot.shape[0])\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Shooting Cost %.3f\" % cost_shoot)\n",
    "print(\"Collocation Cost %.3f\" % cost_col)\n",
    "print(\"Collocation Error %.3f\" % error)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of each dimension across 20 timesteps for the shooting methods.\")\n",
    "print(\"Both methods converge to the origin. Shooting: solid line(-);  Collocation: dashed line(--).\")\n",
    "\n",
    "for i in range(env.dx):\n",
    "    plt.plot(ts, states_shoot[:, i], '-', ts, states_col[:, i], '--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Stability\n",
    " A discrete-time linear system is asymptotically stable if in the presence of no input the system converges towards the zero state. In practice, this means that the absolute value of the eigenvalues of the transition matrix must be smaller than 1. If that is not the case, the system is unstable.\n",
    "\n",
    "For instance, the previous system is stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26052413, 0.14606684, 0.14606684, 0.09743496, 0.09743496])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(np.linalg.eigvals(env.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Theoretical Question\n",
    "Consider the linear system that we currently have, i.e., $$x_{t+1} = Ax_t + B u_t$$\n",
    "and we want to minimize the quadratic cost $$ \\frac{1}{2}\\sum_t x_t Q x_t$$\n",
    "Hence, we have a linear quadratic regulator problem. Derive the gradient update for the action variables for both optimization \n",
    "methods: shooting and collocation. In the case of collocation, do not include the update due to the constraints.\n",
    "\n",
    "Explain in a few lines why the shooting method might become unstable while the collocation method does not.\n",
    "\n",
    "Refer to the pdf for reporting this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Empirical Behaviour\n",
    "\n",
    "Now, we test the effect that you derived and see if the theory matches the empirical behavior. We use the same environment as in the previous part, but we just scale the transiton matrix so it has some eigenvalues larger than 1. Note this is the only change with respect to the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.60524128, 0.22707677, 0.22707677, 0.87169387, 0.87169387])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = LinearEnv(multiplier=10.)\n",
    "np.abs(np.real(np.linalg.eigvals(env.A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired error not necessarily achieved due to precision loss.\n",
      "The optimal cost is 72284018840248.438\n"
     ]
    }
   ],
   "source": [
    "policy_shooting = minimize_shooting(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "The optimal cost is 201.812\n"
     ]
    }
   ],
   "source": [
    "policy_collocation, states_collocation = minimize_collocation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Quantitative Metrics ---\n",
      "Shooting Cost 72284018840248.438\n",
      "Collocation Cost 201.812\n",
      "Collocation Error 0.000\n",
      "\n",
      "\n",
      "---- Qualitative Metrics ---\n",
      "Evolution of the value of each dimension across 20 timesteps for the shooting methods.\n",
      "The shooting method diverges, while the collocation method achieves the desired state. Shooting: solid line(-);  Collocation: dashed line(--).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEBCAYAAACZhwWsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4ZFWd//H3rSVVlbWydZJO7wuHbppusKFpoEEYRFlEXHCGcRm3QWQcUZzf4Og4i8+Mw4ijoziKA4KK6+ioA6IsioLQiIgsNtCcXqHXdGffU6nl/v6oClRSlU5SSVUlnc/reXiout977/nWrdvf3Dp17inHdV1EROT45il2AiIikn8q9iIi84CKvYjIPKBiLyIyD6jYi4jMAyr2IiLzgK/YCYxljDkD+Iy19rxx4hcBf5d66gBbgHXW2u2FyVBEZO5xZtM4e2PM9cA7gX5r7eZJrP+3QLW19hN5T05EZA6bbVf2u4E3A98CMMacDNxE8gq+HXivtbY7FVtE8g/D6cVJVURk7phVffbW2h8B0bRFtwIfTHXp/By4Pi32UeA/rbWRwmUoIjI3zbYr+7HWAF8xxgD4gZ0AxhgP8Hrg74uXmojI3DHbi70F/sJau88YczbQlFq+DnjBWjtYvNREROaO2V7srwHuMMb4ABd4X2q5AfYULSsRkTlmVo3GERGR/JhVX9CKiEh+zJpunEQi4cbjuX3K8Hodct02n5TX1CivqZutuSmvqZlOXn6/tw2on2i9WVPs43GXrq6BnLYNh0tz3jaflNfUKK+pm625Ka+pmU5e9fUVL01mPXXjiIjMAyr2IiLzgIq9iMg8oGIvIjIPqNiLiMwDKvYiIvNAzkMvU5ORfQXYAESAv7TW7kqLXwVcDcSAf7XW3j3NXEVEJEfTGWf/RiBorT3TGLMZ+BxwOYAxphG4FjgNCAKPGGN+kY/piHftfp4f7rmHtpLqUcurYxEuHNgLboJfljXT4asaFa+PDnD+4D5wY9xbvoweb/moeNNwH+cMHcBNRPlZ5SoGPKFR8SWRbjZHDuPGI9wZXsOwUzIqvmKok9OjR0jEh/hJ+GTieEfFVw+1c0q0lURsgB9VvwpITv4z4qTBVk6KdhCJD3Fn9YaM171h4Cgm2kWfO8TPw+tHxRzgVQMtrIp30RWPcn94XXJhmk39h1ka76Y1keDBqrVjtoazBg7QHOvjsOvycNWajPZf3X+Ahng/+3B4rPKEjPb/pH8ftfEB9jg+nqhYNSruAS7se5GqxBDWU8Iz5SvGxB0u6t1NuRvlOW+Q58qWjop7XYdLe3cQJM7TvnJ2lC7CSTt6Htfh8l6Lz0nwhL+SvcGFY7b38Oa+5wF4rKSa/YGGUfFgwuX1fTvAgUcCtRwuGX2/SkU8ykX9ewF4MFRPqz917rnJYxeORbhwcB9uIsEvyxrpGnPu1UUHOC+SPLd+UbaUHl/ZqDe/cbiXs6OHIR7hnvJVDHiDrwRdWBTp4jT3KE50gJ+Vn0TE8Y/a/9KhNk71deIM9nJX+SnERj7Ap9pYGWnl9AVe4i37+UHglOTitPY30MF5axbT9cwTfMN70suvzcUBF84rjXLxps28cO8Puc2/GnBIzrri4LoOl9WE+NOLL2XrNz/HHeUrGHvy/fmSRs47fQsPfO1GflA9+r0HeO9awxlrTuZnX/137qpflRH/0OYzWdfczI9uvoH7F6weFXNw+ZtzzmX1gga+c/O/85sFo7f3ugn+7k8uYElNLbfffCO/W7BiTDzOP154EY1VVdz81f/g6fplo+J+N8anL30DVaFSvvDf/8n2usWj4iVujM9e/haCfj833nITu2tHn3sBN8ZNV1wJwL9+7cvsr25gQW8nX/rIhzNe50ybTrHfAtwLYK19zBhzWlpsE7A1VdwjxphdwHrg9+PtzOt1CIdLp5xEy97Hea5xCc84rxq1vMk9yBv4bwCe4l+wztpR8RXuLi7hawD8ns/ykjP6TV/rbuO13A7Ab3kLR5ymUfFXuY9zPt8E4GHeSbcz+o/NWe5vOCf5Gyw8wNVEnOCoeNS9jzP5Dgkc7nOuy3hdfvdOTuM+XEq537k2I15R831O4V4GqeF+50MZ8fqar7OO+0nQzH3OX2fEF4W/yon8kqizgvucv8qIr675PKvYyiDruN/5QEZ8fc2nWcaT9HI69zuvzoifXvMPLOJ5ujgna/ycmp/SyF7aeG3W+IU1PyDEYVq4PGv89bXfJEQXB7kya/yK2v8mSIQXeTf3OeeNijlugrfXfhGAXXyAB8fEQ24/f1r3XwBYruMxZ8uoeLXbzpvrbwbgOT7B087GUfEm9yCX81UAnhnn3LuUW4Hxz73X8XVg/HPvgtS59TDvynrunc/3APg1f5Vx7iXc+ziXW0gscXjAySwyIfdOTuu8geElpfw6y3tf636f1bvfTs/qGn7jvDMjvsz9Oot++0/0vqqZR50rMuJr49+gedfn6F5fyaOlb8mIb4rcxQldW2lf0sujS07NiF8+0MuWcCkHysI8ajLj74j0c3q4lH3lNVnjH4gNsT5cyt6q+qzxiBMjHC5lV3Vj1rjrS94EtaOuOWs8VOqnqqyUFxqW8PuVJ4/Z2H25zm1vWsYfl56IObQXr9eTU/2bipwnQjPGfA34kbX2ntTzfcAKa23MGPMO4GRr7cdSsTuAO6y1vxxvf9Fo3M31DrJQyJv17jOPJ3lFk0gksm43/bgDOOPGa2rK6e4ezBpPuC4+j/flWOb74OL1etO2zYx7Xo47aftwXs7d4/FkbbuyMkRPz2DWuOu6OI6D67ppcTd9BXBeaSPb9uk5pMcTyWvDl42Nh8OhUe9jxv5dF8fxpMXTjh+vvF8j6aZvP3J4R46zx+PBdcFNJHDTXt/I+h7nlfe+rCJAb+8AuK+8V47jwcUlkUgk90MidYWc2j6VSjQWezklN5HcNnkIPZBwicfjuInU5xLXwXWT+3Hw4LrJeDyewI27xOMJEq5LIu4mX1AigdfvY3BwCDfukoi5uImRGCTiUaKxOG48hjvy/0QcJx7HSbi4iTgeN/nYcRN43NT/cfG7Lv7U/wNAiQMljovPAzhxcOIknDh4ErhOHNeJg5Mg7sSJEWOYGINOlCFiDDtRok6UhHcY1z8E/mE8/hgefxSvP4HX7+L3JfD5wOdJ4NBPPN6OS5yxPE4ZPl8tJf46vN46fP5afL5a/L56fL5aAoHllJQ0jXvuj9ypOl583HNvBuPZ6kpNTfl07qD9A8lelGOazpV9D1CR9txjrY2NE6sAuqbR1jEFAgECgcwTo9j8fj8+X3TiFfPI6/VmLAsESvD7Y+PGiyUYCBEMzPS8JdN/feGqUnBn51iGQt7+77oukeE4fT0R+vuGGeqNMtwfJToQIzYYwx2K4UbieAajVA5EWRz1EEobA9LJMJ3OMN1ujN5Ygv5hH9HhUMZlzGBJL8ONnTSucjjJ1FFf4SMaayMWaycWbSUaaycWa2MoYon1t5NI9I3avqRkCRXlmykv30x5+el4va+UopGLoJHHx1Ls+EybTrHfClwG/CDVZ78tLfY48GljTBAIkPzFqWen0ZaIFJnjOAQDPoL1Purqy465bjhcSntHH0cO9NL6Ug+DLf34Oj1U9/s52QWfzwEfDJe6HPXG6QxE6QtE6fdHGOgfIHiojsF9pTzxK+gqb8FpjrD4hKWcdtIbWFhWO6qtRGIw+Qcg2srA4HP09T1GZ9dPae/4AeCltHQd5eWbqSjfTGXlpjweodltOt04I6Nx1pP83P4e4BJgl7X2rtRonPeT/D7u31K/Lzuu6XTjHI+TG+WT8pqa2ZoXzN7cjpXX0GCUQy9107G/l9iRQYI9w9QNudSnvuDuw2VvUwDvCbD74AF69sYIdVTjdX1EPcN0VR8kuMRl5ZpmNq7cQEVJRUYbiUSUgYFn6Ot7jN6+xxgcfI5k12gZZaWnJ6/6KzYTKFmK4zgZ2xfaNCdCm1Q3zqz58RIV+8JRXlMzW/OC2ZtbLnl1dQ5xYGcHg0+2srY/gQPsCvuo39JM9dJy/vjcC+yxh4ns8xLqT45w6ivpore+harlJZy0dgUbFm6gxFuSse9YrJu+/scZHn6CjvZHGI4eBMDvb0xd9Z9JeflmfGNGThWKiv0kHU8nfCEor6mZrXnB7M1tunnte6mblx7cz9q2KOU47A86+DfWs2RTE47Hoau9j2eefYGDOztwD4XwR4O4JDhSu4ez33wiG7OM4knPKxLZ//JVf3/f48QTvThOkMbGD1JX+zYcp7DfZanYT9LxesLni/KamtmaF8ze3GYqr7bOQbb98iWW7RtgIR46vNB3YhUrXr0Ybyj5lWMi4XJ4fzvPPL2Drich5hkmvrmFd1z4Fvye0fcgZMvLdWMMDD7P0aO30tv7MKHQSSxq/kdCITPt/CdLxX6SjvcTfqYpr6mZrXnB7M1tpvPqH4ry+EP7qNzezbq4hyFcjiwpY+n5iwnUvXLD49EjHdz33ScJdFRxuMlyyVs3szrtxqxj5eW6Lt3d93Po8GeIxbqpr38XDQvej8cTzLr+TCpEsZ+d48lERNKUBf2c/7qVbPjQKTx+dh1/CLgs3NdP4psvsPsbz9G3uxvXdVnQUMPbr72Aik1RGg6v4uGv7uWHW+8k4Y4/Jn6E4ziEw6/jhNU/obr69bS23s6OnW+lr2/ce0HnFBV7EZkzfF4P52xexPkfPBV7ySJ+WelQ3h7B/397OXjzNjqfb8fjdbj4srM4471L8Pk9uPcu5L++/i1aeo9Mrg1fFYsXfYrly5N34O/ZexX7D/wzsVh3Hl9Z/qnYi8ic4zgOG9fUc9lVp9B95SrubvQxOBjFf88+Ond0ALB8+UL+/LpX41vbR+Petdz1X7/np7+7b9JtVJSfwQmrf0h9/Xvo7PwpO3a+ia6u+7Lc7T43qNiLyJx2QnMlb3n7OhJ/upKDJHDvfonBI/0A+AM+3vzn57P2T2sIxcs58B0fX/n+t+iJTO4q3eMJ0tT4YVav+g5+fyP79n+MF1/6MMPDLfl8SXmhYi8ixwWzOEzXhYsYcl26v7+TeN/wy7F1J6/kio9sxrt8kLrnTuTbNz3A73ZPvi8+FDqRVSvvoKnxb+jre5wdO99MW9v3cN3ZN03LeFTsReS4ccb6Bp7ZWEMw5nLoWy/gRl8pxqHyIFd95FKWXBKgaqCeXXdEuO2n3yMSG5rUvh3HR339OzEn/IjS0g0cOvwZdu9+D0NDuybeeBZQsReR48pFr17KL5aHqB2Is/+7NjkTaIrjOGw+cz2XfGgDiboBKh5fxa1f/j+2H35h0vsvKWlm+bKvsHjRp4kM72PnritpOfIV3EmM+CkmFXsROa44jsOb3mi4q9ZDQ9swB+/ak/GlanVNJe/44IXUnJugtn0pj99yiB8+9NMptVFdfSnmhB9TVXkhR4/eQkfH/870S5lRKvYictzxeRwue9tJ3BNKUL+7l6O/OZixjuNxeM2FZ3Du1SuhLEbsgXqe3P3M1Nrx1bB48b9RVnY6LS1fIhbrmKmXMONU7EXkuFRW4mPz29ew1Run8olWura1ZV2vuXkBl1+1mYQnxmM/3zmpG7DSOY5D88KPk3AHOdzyhZlIPS9U7EXkuNVYFaLpilVsJ4H3/v307cn+G0rhqkrKT4tSd3Qp9//+wSm3EwyuoK7unXR23kV//1PTzDo/VOxF5Li2ZlEVA69bxBESHL3jeeId2UffXPLacxgM9bDvVwMMRSc3Qiddw4L34/c3cvDQv+G6sYk3KDAVexE57m1Z18Czp9cRjSdo/+4O3IHMYuwv8bHi/CrC/Q38+BeTv9N2hMcTYmHT9QwN7aSt/XszkfaMUrEXkXnh8nOW8IsTywlG4hz5nsWNZfbNb9m8kf6aNqJPVHC0p3XKbVRWnk9F+dkcOfJVotGjM5H2jFGxF5F5wXEcrr5yPd+v81DdFeXoj3ZlDMl0HIfNrz+BULSCO+/+dU5tLFz4d7hulEOHPz9Tqc8IFXsRmTf8Xg9vu/IkvlcaJ3xggI7792Wss2b1SoaWtlL2wiKe32+n3EYgsJgF9e+lu/teevt+NxNpzwgVexGZV8oDPl7752v5mS9G+bOd9D6eOfXxRW88Axx46O5ncprlsr7+PZSULObQoRtIJIYn3qAAVOxFZN5ZGA5xwhWr+a0Tw/fwYSI7Rw/JXFBXi399P7WHlvPQH3875f17PAEWLvwYkciLtLV9a6bSnhYVexGZl9Y1V+FetJidxBn+6YvEW/pHxS+75FyGSvqwv2glGo9Oef+VFVuorLyAI0dvZXj40EylnTMVexGZt85f28Dzm2rpcBN0/3AXbvyVLptgKEDTliDV3Qu586H7c9r/wqa/BeDQ4RtnJN/pyKnYG2NCxpgfGWMeNsb83BhTn2WdzxpjfmuM+b0x5qrppyoiMvP+bMtSHmkOUDbs0rOrc1TsgnPPoL+yg55H/XQNZr/79lhKShppaLianp4H6en5zUylnJNcr+yvAbZZa88B7gA+mR40xpwPrLLWnglsAT5mjKmeVqYiInngOA6bzl3MAC5Hnxo9tt7j9XDKRUsoj1Tzk5/9Mqf919W+g0BgBYcOfYZEYup35s6UXIv9FuDe1ON7gNeMif8WeG/qsQt4gal3eomIFMCqpgqe9ieoOTw4av57gFNPXsPAwlb8zzaw5+iLU963x+OneeHHGY4e5Gjr7TOU8dT5JlrBGPM+4Loxi48AIz/i2AtUpQettUPAkDHGD3wTuMVa23esdrxeh3C4dLJ5j9nWk/O2+aS8pkZ5Td1szW0u5jW8qory7X24R4aoXlM7Knb528/kvv+w/PJnj3P9dWun3G44/Gp6+y6jtfXrLF1yBaHQsknnNVMmLPbW2tuA29KXGWN+DFSknlYAGZ1ZqW6b/wUetNbeMFE78bhLV9fAZHLOEA6X5rxtPimvqVFeUzdbc5uLeS1eX8/g9l72P7wPT1NoVKy6shr3xC4qty/lvt89zBlm45Tbrqu9lvb2X7P9hX9m+bIv4zjOpPKaSH19xcQrkXs3zlbgktTji4GH04PGmBDwAHC7tfZfcmxDRKRg1jRX8pQvQfhQZlcOwOsvO4eYL8KT97xELDH1WS39/noaG66hr+9RenoemImUpyTXYn8zcJIx5hHg/cCnAIwxNxpjNgEfAFYAVxljHkz9t3xGMhYRyQPHcehdXEZFHAb2dmfEKyrKCJ/hUtu+mHsem/q8OQC1tX9GMHgChw5/lni8sJ98JuzGycZaOwC8Ncvy61MPHwf+cxp5iYgU3LKNDQztfZG2J4+ycmU4I37xBVv41lO/ouehGP0b+ykLlE1p/47jo3nh37N7z7s4evQWmpo+MlOpT0g3VYmIpKxbEuZJb4KqgwNZu3K8fi/mwnoqB+r4yf253WhVVraB6uo30tr2bYaGdk035UlTsRcRSfE4Dj2LS6mIw+BLvVnX2bxxA/31rSSerOZQx+Gc2mlqvBavt5SDh27IaaK1XKjYi4ikWXJqAxFcWp7MnA0Tkn3751x2EoFYiLvvzu2uWJ+vhsbGa+nv/wNdXT+fTrqTpmIvIpJm/bJqnvQmqNjfP+5V96rlS4mubKdi1xL+uPe5nNqpqX4zodA6Drd8nlgs+6eImaRiLyKSxutx6FxYSmUcIvvGL8IXv+EsEk6CR372fE5dMY7jobn5E8TjPfT0PjOdlCdFxV5EZIxFr1qQ6soZ/3dka2vClJ46RN2RZTzwh9y6c0pDa1m75gFqqrfkmuqkqdiLiIxx6vJqnvLEKdvXd8yr9ksvPofBYC97ftVDJBrJqS2vtzLXNKdExV5EZAyf10PbwhCVMRg+MP60XiWBEprOKiHc28jvt/+xgBlOnYq9iEgWzac2MIxLyx+yj8oZsfGUkwDYt7elEGnlTMVeRCSLjStreNITJ7Rv/FE5ALXV1QwEu+k9NDt+WHw8KvYiIln4vR5aG0NURV1iB/uPua5bN4i/vaJgN0jlQsVeRGQcjRsWEMXl8Dg3WI2oag5RHqnmpbb9Bcps6lTsRUTGcfoJtTzlxAm+eOxROStXNAOwfcfuQqU2ZSr2IiLjCPg8HG4IJrtyDo8/JfGJK1aSIM7hfR0FzG5qVOxFRI6hcUM9MVyOHKMrJxAsYaCyi+Ejs7ekzt7MRERmgdNNHU86cUr29h6zK8e/IE5ZVy2R6FABs5s8FXsRkWMI+b0cqg9QNeySODJ+V07DkioC8RDP7bMFzG7yVOxFRCZQv76O2ARz5ZhVKwDYvftAodKaEhV7EZEJbDqxnqeJ49/TM25XzpLmJqLeCB0Hjj0mv1hU7EVEJlAe8LG/PkBVxCVxdDDrOo7HYbimB9qCBc5uclTsRUQmoebkWmK4HH1q/K6csiYvlX0LaO1rLWBmk6NiLyIyCZvX1PMMcTy7xu/KWbysAa/rZduu2fclrYq9iMgkVAb9vFjjJxxJkGjN3pWzdvUqAPbvHf/qv1hU7EVEJqn65FriuLQ9nb2bpipczlCwl/6WWIEzm5gvl42MMSHg28ACoBd4l7U249UbY0qBR4G/s9beO51ERUSK7cy1DfzxoRaW7+zGvdDFcZzMleqHCBytJO7G8Trewic5jlyv7K8BtllrzwHuAD45znpfBmbvnJ8iIlMQLvWzu9pPeChBoi17V051cxkVkVp2Hpldk6LlWuy3ACNX6vcArxm7gjHm/5G8qs//z6aLiBRI1boaEri0P9OWNb5y5WIA7M49hUxrQhN24xhj3gdcN2bxEaA79bgXqBqzzQXAamvt1caYsyeTiNfrEA6XTmbVLNt6ct42n5TX1CivqZutuR3PeV101jL+8PARlu/sJnxF5r5OO+VE/vid39B2qHfSbRXieE1Y7K21twG3pS8zxvwYqEg9rQC6xmz2PmCpMeZB4ETgVcaYFmvt0+O1E4+7dHWNP+/EsYTDpTlvm0/Ka2qU19TN1tyO57x8wM6wj1O64nTs7sBTm3kTVaSyh8ghZ9JtTSev+vqKiVci926crcAlqccXAw+nB621b7PWnm2tPY9kd8/1xyr0IiJzScXaZFdOxzijckoaXCq66+kd7i1wZuPLtdjfDJxkjHkEeD/wKQBjzI3GmE0zlZyIyGx05roGthEntmNsp0ZSw5JqAvFSnn3phQJnNr6chl5aaweAt2ZZfn2WZe/OpQ0RkdlqQUWA+yp9bOiJk+gYwlMzuitnzeoVPPzAXvbsOcCZq08vUpaj6aYqEZEclK2tBqAry6ichqZaYt5hug7Mnh8yUbEXEcnB5nUN/JEYw7YzI+bxOERr+vC0h47561aFpGIvIpKDhVVBtpd7qO6Pk+iMZMQrFvoJ9zVwoGd2/JiJir2ISI6CJya7cnq2ZXblLFnWgNf18dyuHYVOKysVexGRHG1e38hzxBja3pEROzH1M4UHX5odc9ur2IuI5GhJdYhtZR6q++K43aO7csrDISLBfgYOJ4qU3Wgq9iIi0xBenpwtputgX0bMqY8Q6qxmOJ7Zp19oKvYiItOwLFXs21syf2i8blE5lZE6trcU/5erVOxFRKZhQUMZEVyiHZlj6letXAKA3f1igbPKpGIvIjIN9RVBjpDA6R3OiC1dthCXBG37e4qQ2Wgq9iIi0+DzOHR4HQID8YyYP+AlUtVL/Ki/CJmNpmIvIjJN/UEPlcPZ75QNNjhUdTfQPpT9x04KRcVeRGSaomU+yhPgDmde3TctrSEYL2Pbi9uLkNkrVOxFRKbJUxUAYDjLl7QnrloOwIt7DhU0p7FU7EVEpilQmyz2XUcyf22qtrGKmHeYnkPFHWuvYi8iMk2VC8oA6GvNLPYej0O8dgBvezlxN7Obp1BU7EVEpmlBfSmDuESzzH4JULUwQE1/E3s69xQ4s1eo2IuITNOCyiAtJPBkGWsPsGx5E17Xx/O7izcDpoq9iMg0+TwOnT6yjrWHV+6kPbwvc3bMQlGxFxGZAX0B77hj7UurAgwHBxhqKXBSaVTsRURmQLTcR6kL7lAsa9y7IEpZZy190czZMQtBxV5EZAZ4qkoAxv2Stn5RJVWRep5vKc7NVSr2IiIzIFgbAqDrSOZUxwCrVy0FYGeRZsBUsRcRmQHhBaUA9LUOZo0vXFKHS4L2/cXpxvHlspExJgR8G1gA9ALvsta2jlnn3cA1gBe401r7L9NLVURk9lpQV0ofLrHOzCkTIDkDZrSyn0RrANd1cRynoPnlemV/DbDNWnsOcAfwyfSgMWZlap3zgE1AiTGm+HN8iojkychYe6c3Ou46oSYPNb0LOdh/oICZJeV0ZQ9sAW5MPb4H+Icx8dcATwDfBJqAT1trxz8CgNfrEA6X5pSM1+vJedt8Ul5To7ymbrbmNl/zesznsGwoMW4bK81Cdthudh/dzbpFpmB5wSSKvTHmfcB1YxYfAbpTj3uBqjHxOuBc4CwgBDxijNlkre0ar5143KWrK3NeickIh0tz3jaflNfUKK+pm625zde8+oMeKvsTdHb2Z+2mWbKoiR10s2P7frpWvJLHdPKqr6+Y1HoTFntr7W3AbenLjDE/BkZaqADGFvF24EFrbS/Qa4zZDpwAPD6prERE5qBouZ9AXxQG41CaWV7DC8qJe6P0HjpmR0de5NpnvxW4JPX4YuDhLPHzjDFBY0wZsBbYlWNbIiJzgvflsfbZv6T1eB3c2iFKOioZjhd2yuNci/3NwEnGmEeA9wOfAjDG3JjqrtlG8tPAVpJ/CP7FWlu8SSFERAogWBsEoHucsfYAVc1BavubsR22UGkBOX5Ba60dAN6aZfn1aY+/AHwh99REROaWcEMZ0E5f2yD146yzYkUz2546yvY9uzm5fn3BctNNVSIiM2RBbYhuEsTGmTIBYOnyJgCO7O8sVFqAir2IyIxpKA/QgovnWGPtK/1Eg0NEWubGTVUiIjKGz+uhwwfBwfF/ftBxHPwLYlR1N9A+1Faw3FTsRURm0EAwOa+962af2x5gwZIwVUP1PNfyXMHyUrEXEZlBsXIfJQD92ee1B1id+uWq3XsKN22Cir2IyAzyVAWA8cfaAyxYFMbFpeNA4e4yVrEXEZlBobrkvPbdR8cv5P6gl3jVAE5bkLg7fv/+TFKxFxELlE7yAAAOtUlEQVSZQSPz2ve3ZZ/XfkRpk4/a3kXs7dlTiLRU7EVEZlJjbYgOEsSPMdYeYPGyBYRi5Ty/74WC5KViLyIygxrKAxwmccyx9gArljcDsP+lo4VIS8VeRGQm+bweunzOMcfaA1Q1lBL3xug/pD57EZE5qT/kpTLq4ibGH2vv8To4tRFCndX0DvfkPScVexGRGRYv9yVnmew7dldO9aIy6voXse3Is3nPScVeRGSGeScx1h5g1YpFeF0fbnsg7zmp2IuIzLDJjLUHaFpei8fr0OxZmvecVOxFRGbYyFj7gYnG2leWcMl1J7PkpNq856RiLyIywxqrQ7SSIN41POG6pVUBPN78T3esYi8iMsMaKkbG2k9c7AtFxV5EZIb5vR46fQ6hocKMoZ8MFXsRkTwYDHmpjIIbH3+sfSGp2IuI5EG83JcssH2zoytHxV5EJA+84eTY+WP9+HghqdiLiORBae3kxtoXii+XjYwxIeDbwAKgF3iXtbZ1zDqfB7YACeBvrLVbp5mriMicEW4oJY7L4ARj7Qsl1yv7a4Bt1tpzgDuAT6YHjTEbgLOAM4B3AjdNJ0kRkbmmKRziKC6xrrndjbMFuDf1+B7gNWPiB4EBIABUAseeDUhE5DjTUBGghQTeCSZDK5QJu3GMMe8Drhuz+AjQnXrcC1SNicdIdt+8kIpdNVE7Xq9DOFw60WrjbOvJedt8Ul5To7ymbrbmprySuvwOq4YSE7ZZiLwmLPbW2tuA29KXGWN+DFSknlYAXWM2+wugBXhdKv6IMeYxa+2B8dqJx126unL7IiMcLs1523xSXlOjvKZutuamvJIGg14qel062/pwfON3pEwnr/r6iolXIvdunK3AJanHFwMPj4l3An3W2jjJK/8IUJZjWyIic1Kswg+A21P8sfY5jcYBbga+aYx5BBgG3gZgjLkR+F/gu8DZxphHAS/wHWutnYF8RUTmDF84AIeixLsieGqCxc0ll42stQPAW7Msvz7t6QdyTUpE5HhQVhcC+ug5OkDtirFfbRaWbqoSEcmTcH2IKC4D7cf+xapCULEXEcmTheEQR3CJz4Kx9ir2IiJ5MjLW3jcLxtqr2IuI5ElyXnsoHSz+vPYq9iIieTRU6qMsDm40UdQ8VOxFRPIoXj47xtqr2IuI5JG3OjmvfbG/pFWxFxHJo/K65M1UPUWe117FXkQkj2rqSongMthe3HntVexFRPJoYThECwkS6sYRETl+NVQEOIyLty9W1DxU7EVE8sjv9dDlg7Kh4o61V7EXEcmzoVIfoTi4keIVfBV7EZE8S8yCee1V7EVE8sw3C8baq9iLiORZWW0IgN4ijrVXsRcRybPauhADRR5rr2IvIpJnC6tSY+271WcvInLcSo61T+Ar4lh7FXsRkTwr8Xno9juUDsVxXbcoOajYi4gUwGDIRzABFOnmKhV7EZECKPZYexV7EZEC8Bd5rL2KvYhIAZTVJcfa97UWZ6y9bzobG2PeBLzVWvu2LLGrgKuBGPCv1tq7p9OWiMhcVl9XSi8uw+1DVBeh/Zyv7I0xXwRuyLYPY0wjcC1wNvA64AZjTCDXtkRE5rqFlUEOF3Gs/XSu7B8F/o/k1ftYm4Ct1toIEDHG7ALWA78fb2der0M4XJpTIl6vJ+dt80l5TY3ymrrZmpvyyhQqD3IXCRr6Yxk5FCKvCYu9MeZ9wHVjFr/HWvs/xpjzxtmsEuhOe94LVB2rnXjcpasrt76scLg0523zSXlNjfKautmam/LKrsvvEBqM09nZj+M4M5JXfX3FpNabsNhba28Dbpti+z1AegYVQNcU9yEiclyJhHyU9ACDMSj1F7TtaX1BewyPA582xgSBALAGeDZPbYmIzAmJSj/0RHG7h3EKXOxndOilMeajxpg3WGtbgJuAh4FfAX9vrR2aybZEROaaktRY+1gRxtpP68reWvsg8GDa88+nPb4VuHU6+xcROZ6U1oWAPvpbBylZU9i289WNIyIiYzTUlNJFgkRH4ee11x20IiIF0lQV5DAublfhx9qr2IuIFEhjRYAWEvj6Cz+vvYq9iEiBlPg8dPkdyiKJgs9rr2IvIlJAkVIfPhco8NW9ir2ISAElKpLjYtzuwg6/VLEXESmgkpogALECf0mrYi8iUkDldckJzwYKPK+9xtmLiBRQY3WINhJ4Owo7qYCu7EVECqipKkgLCdwCz2uvYi8iUkCNFQEO4+LXaBwRkeNXic9Dt9+hNJLATRRurL2KvYhIgUVKfXgB+qIFa1PFXkSkwNzK5Fz2hey3V7EXESmwkXnt4wWc117FXkSkwCrqS0ng0t9WuLH2KvYiIgXWGA7SikukQ1f2IiLHrabKwo+1V7EXESmwxsogh0lQ0q/ROCIix62Az0O330No2MWNJwrSpoq9iEgRREp9yQLcW5irexV7EZFiKPBYexV7EZEiGJnXvlBj7ac1xbEx5k3AW621b8sSuw64MvX059baT02nLRGR40llXYgYPUTbBqkvQHs5X9kbY74I3JBtH8aYFcDbgbOAzcBrjTHrc21LROR40xQOchSX4QLNaz+dbpxHgWvGie0HLrLWxq21LuAHCjtTv4jILNaUGn7p9hSmz37CbhxjzPuA68Ysfo+19n+MMedl28ZaGwXajDEO8FngKWvtjmO14/U6hMOlk8s6Y1tPztvmk/KaGuU1dbM1N+U1sVBZgD+QYO1AvCB5TVjsrbW3AbdNdcfGmCBwO9AL/NVE68fjLl1duc0TEQ6X5rxtPimvqVFeUzdbc1Nek9PtdwgNJ4gNxejuz63zo76+YlLr5eU3aFNX9HcCv7LWfiYfbYiIzHWRUj90p0bk+PPb1owWe2PMR4FdgBd4NRAwxlycCn/cWvvbmWxPRGQuc6r80B0l3jUE9YG8tjWtYm+tfRB4MO3559PCwensW0TkeOerDRLbN0x8KJ73tnRTlYhIkVTXhvgL+uluzv+Xxir2IiJF0lQVZB8JDvbm/y5aFXsRkSJpqkz2dh/sGsx7Wyr2IiJFsqQ6xPvPXMqmZdV5b0vFXkSkSDyOw1VnLaWpKpT/tvLegoiIFJ2KvYjIPKBiLyIyD6jYi4jMAyr2IiLzgIq9iMg8oGIvIjIPqNiLiMwDjuu6xc5hRCvwUrGTEBGZY5bCxL9ZPpuKvYiI5Im6cURE5gEVexGReUDFXkRkHlCxFxGZB1TsRUTmARV7EZF5wFfsBKbCGOMBvgJsACLAX1prd6XFrwKuBmLAv1pr7y5QXn7gdmAZEEi1fVda/DrgL0neSwBwtbXWFii3J4Ge1NO91tr3pMWKdbzeDbw79TQInAI0Wmu7UvEvAluA3tQ6l1tru/Oc0xnAZ6y15xljVgHfAFzgWeCD1tpE2roh4NvAglSO77LWtmbudcbzOgX4EhAnef7/hbX2yJj1x32/85jXqcDdwM5U+GZr7f+krVus4/V9oDEVWgY8Zq29Mm1dBziQlvdvrbUfn+F8MmoD8DxFOL/mVLEH3ggErbVnGmM2A58DLgcwxjQC1wKnkSwgjxhjfmGtzf8v+cI7gHZr7TuNMTXA08BdafGNJP9h/qEAubzMGBMEHGvteVliRTte1tpvkDzZMcZ8Gbh9pNCnbAReZ61ty3cuqRyuB94J9KcWfR74pLX2QWPMV0meYz9J2+QaYJu19p+NMVcCnwQ+XIC8vgh8yFr7tDHmauBjwEfT1h/3/c5zXhuBz1trPzfOJkU5XiOF3RhTDfwauG7MJiuBJ621l810Lmmy1YanKcL5Nde6cbYA9wJYax8jWahGbAK2WmsjqavAXcD6AuX1Q+AfUo8dklfK6TYCHzfGPGKMmdErhwlsAEqNMfcbY36V+gM5opjHCwBjzGnASdbaW9KWeYDVwC3GmK3GmPcWIJXdwJvTnm8EHko9vgd4zZj1Xz4Px4nnK68rrbVPpx77gKEx6x/r/c5nXhuBS40xvzHG3GaMqRizfrGO14hPAV+y1h4es3wj0GyM+bUx5ufGGJOHnLLVhqKcX3Ot2FcC6R/n48YY3zixXqCqEElZa/ustb2pk/x/Sf4lTvd94APAnwBbjDGvL0RewADwH8DrUu1/ZzYcrzSfIPkPMV0Zya6KdwAXAX9ljMnrHyFr7Y+AaNoix1o7cmt5tuOSfuzydtzG5jVSrIwxZwF/DfznmE2O9X7nLS/gceBvrbXnAnuAfxqzSVGOF4AxZgFwAalPkmMcBm6w1p4P/BvJrpOZzilbbSjK+TXXin0PkH7V4LHWxsaJVQDpXQN5ZYxZTPKj4restd9NW+4AX7DWtllrh4GfAacWKK0dwLetta61dgfQDjSlYsU+XmHAWGt/PSY0AHzRWjtgre0FfkXyirWQEmmPsx2X9GNX6OP2Z8BXgUuz9OMe6/3Op5+kdVH+hMzzu2jHC7gC+K61Np4l9gRwJ4C19hFgYerf64zKUhuKcn7NtWK/FbgEIPURdVta7HHgHGNM0BhTBawh+eVH3hljGoD7gY9Za28fE64EnjXGlKdOpD8BCtV3/16S32tgjFmYymXko2zRjlfKucADWZafAGw1xnhTX25tAZ4sYF4ATxljzks9vhh4eEz85fNwnHheGGPeQfKK/jxr7Z4sqxzr/c6n+4wxm1KPLyDz/C7K8Up5DcmukGz+CfgIgDFmA7A/7Yp7RoxTG4pyfs21L2h/AlxojHmUZP/Xe4wxHwV2WWvvMsbcRPLAeIC/t9aO7dPMl08A1cA/GGNG+uduBcqstbcYYz5B8i97BHjAWvvzAuV1G/ANY8wjJL/5fy9wrTGm2McLwJD8yJ98Mvp9/BbwGMmP5HdYa58rYF4AfwPcaowpAbaT/PiNMeZ+4PXAzcA3U8d1GHhbvhMyxniBm4B9wI9T3csPWWv/yRhzB8nugYz3O+2Tbz5dA3zJGBMFWoD3p3Iu2vFKM+o8G5PXvwPfNsZcSrIv/d15aD9bbfgwcFOhzy/NeikiMg/MtW4cERHJgYq9iMg8oGIvIjIPqNiLiMwDKvYiIvOAir2IyDygYi8iMg/8f9JDvZnQ6SORAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_shoot, states_shoot = rollout(env, policy_shooting)\n",
    "cost_col, states_col = rollout(env, policy_collocation)\n",
    "states_shoot, states_col = np.array(states_shoot), np.array(states_col)\n",
    "error = np.linalg.norm(states_col[1:,:] - np.array(states_collocation))\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Shooting Cost %.3f\" % cost_shoot)\n",
    "print(\"Collocation Cost %.3f\" % cost_col)\n",
    "print(\"Collocation Error %.3f\" % error)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of each dimension across 20 timesteps for the shooting methods.\")\n",
    "print(\"The shooting method diverges, while the collocation method achieves the desired state. Shooting: solid line(-);  Collocation: dashed line(--).\")\n",
    "ts = np.arange(states_shoot.shape[0])\n",
    "for i in range(env.dx):\n",
    "    plt.plot(ts, states_shoot[:, i], '-', ts, states_col[:, i], '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Open-loop vs. Closed-loop\n",
    "Until now, we have been optimizing directly the sequences of actions and then applying each of the actions in the sequences \"blindly\". While this suffices in deterministic environments, in the presence of noise it does not work out well usually. Because of the stochastic transitions, the state that you encounter at a specific time-step differs from the one predicted by the optimzation problem; as a result, the action found is no longer valid. In stochastic environments, we need close loop controllers in the form of either (i) parametric policies (e.g. linear feedback controllers or neural-networks), or (ii) non-parametric policies (e.g. model predictive control).\n",
    "\n",
    "In the following, we will compare the different behaviour of open-loop and closed-loop control methods. Use the optimal cost for the action optimization methods to check the validity of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Optimization -- Shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired error not necessarily achieved due to precision loss.\n",
      "The optimal cost is 0.010\n"
     ]
    }
   ],
   "source": [
    "action_shooting = minimize_shooting(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Optimization -- Shooting\n",
    "\n",
    "We will start by learning a neural network policy using a shooting method. Fill in the code for ``eval_policy``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(env, policy, params):\n",
    "    \"\"\"\n",
    "    Find the cost the policy with parameters params.\n",
    "    Use the function step of the environment: env.step(action). It returns, next_observation, cost, done,\n",
    "    env_infos.\n",
    "    \n",
    "    You can set the parameters of the policy by policy.set_params(params) and get the action for the current state\n",
    "    with policy.get_action(state).\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_cost = 0\n",
    "    horizon = env.H\n",
    "    \n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "The optimal cost is 0.010\n"
     ]
    }
   ],
   "source": [
    "def minimize_policy_shooting(env):\n",
    "    policy_shooting = NNPolicy(env.dx, env.du, hidden_sizes=(20,))\n",
    "    policy_shooting.init_params()\n",
    "    params = policy_shooting.get_params()\n",
    "\n",
    "    res = minimize(fun=# Fill this with a function that returns the cumulative cost \n",
    "                       # given just the parameters of the policy,\n",
    "                   x0= # Fill this with the inital parameters,\n",
    "                   method='BFGS',\n",
    "                   options={'xtol': 1e-6, 'disp': False, 'verbose': 2})\n",
    "    print(res.message)\n",
    "    print(\"The optimal cost is %.3f\" % res.fun)\n",
    "    params_shooting = res.x\n",
    "    policy_shooting.set_params(params_shooting)\n",
    "    return policy_shooting\n",
    "\n",
    "policy_shooting = minimize_policy_shooting(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Predictive Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCPolicy(object):\n",
    "    def __init__(self, env, horizon):\n",
    "        self.env = env\n",
    "        self.H = horizon\n",
    "        self.env = copy.deepcopy(env)\n",
    "        np.random.seed(1)\n",
    "        self.init_actions = np.random.uniform(low=-.1, high=.1, size=(horizon * env.du,))\n",
    "        \n",
    "    def get_action(self, state, timestep):\n",
    "        \"\"\"\n",
    "        Find the cost of the sequences of actions and state that have shape [horizon, action dimension]\n",
    "        and [horizon, state_dim], respectively.\n",
    "        Use the function step of the environment: env.step(action). It returns, next_observation, cost, done,\n",
    "        env_infos.\n",
    "\n",
    "        In order to set the environment at a specific state use the function self.env.set_state(state)\n",
    "        \"\"\"\n",
    "        env = self.env\n",
    "        horizon = min(self.H, env.H - timestep)\n",
    "        \n",
    "        def eval_mpc(actions, state):\n",
    "            actions = actions.reshape(horizon, env.du)\n",
    "            total_cost = 0\n",
    "            \"\"\"YOUR CODE HERE\"\"\"\n",
    "\n",
    "            \n",
    "            \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "            return total_cost\n",
    "\n",
    "        self.init_actions = np.random.uniform(low=-.1, high=.1, size=(horizon * env.du,))\n",
    "        res = minimize(fun=lambda x: eval_mpc(x, state),\n",
    "               x0=self.init_actions, \n",
    "               method='BFGS',\n",
    "               options={'xtol': 1e-6, 'disp': False, 'verbose': 2}\n",
    "              )\n",
    "        act_shooting = res.x\n",
    "        return act_shooting[:env.du]\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpc_policy = MPCPolicy(env, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### No noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.\n",
    "cost_act, states_act = rollout(env, action_shooting, noise)\n",
    "cost_pi, states_pi = rollout(env, policy_shooting, noise)\n",
    "cost_mpc, states_mpc = rollout(env, mpc_policy, noise)\n",
    "states_act, states_pi, states_mpc = np.array(states_act), np.array(states_pi), np.array(states_mpc)\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Action Cost %.3f\" % cost_act)\n",
    "print(\"Policy Cost %.3f\" % cost_pi)\n",
    "print(\"MPC Cost %.3f\" % cost_mpc)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of the angle and angular velocity of the cart-pole environment across 20 timesteps for the open-loop, policy controller, and mpc controller.\")\n",
    "print(\"All the approaches achieve the same cost and follow the same trajectory. Open-loop: solid line(-);  Policy: dashed line(--). MPC: dotted line(.)\")\n",
    "ts = np.arange(states_act.shape[0])\n",
    "plt.plot(ts, states_act[:, 0], '-', ts, states_pi[:, 0], '--', states_mpc[:, 0], '.')\n",
    "plt.plot(ts, states_act[:, 2], '-', ts, states_pi[:, 2], '--', states_mpc[:, 2], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 1.\n",
    "cost_act, states_act = rollout(env, action_shooting, noise)\n",
    "cost_pi, states_pi = rollout(env, policy_shooting, noise)\n",
    "cost_mpc, states_mpc = rollout(env, mpc_policy, noise)\n",
    "states_act, states_pi, states_mpc = np.array(states_act), np.array(states_pi), np.array(states_mpc)\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Action Cost %.3f\" % cost_act)\n",
    "print(\"Policy Cost %.3f\" % cost_pi)\n",
    "print(\"MPC Cost %.3f\" % cost_mpc)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of the angle and angular velocity of the cart-pole environment across 20 timesteps for the open-loop, policy controller, and mpc controller.\")\n",
    "print(\"In the presence of noise, the open-loop controller fails to stablize the pole, while the policy and mpc controller succeed. The MPC approach achieves the best performance. Open-loop: solid line(-);  Policy: dashed line(--). MPC: dotted line(.)\")\n",
    "ts = np.arange(states_shoot.shape[0])\n",
    "plt.plot(ts, states_act[:, 0], '-', ts, states_pi[:, 0], '--', states_mpc[:, 0], '.')\n",
    "plt.plot(ts, states_act[:, 2], '-', ts, states_pi[:, 2], '--', states_mpc[:, 2], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the MPC method perform better than having a policy?\n",
    "Is there anyway we could make the performance of the policy better?\n",
    "\n",
    "Reply in no more than 5 lines in the box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reponse:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Optimization Methods\n",
    "In the previous parts, in order to optimize the collocation methods, we have used a built-in constrained optimization algorithm. Here, we implement our own by using the merit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merit_function(env, mu, x):\n",
    "    \"\"\"\n",
    "    Implementation of the merit function. We use the previously defined functions eval_collocation and constraints\n",
    "    to obtain the cost and error of the variables. Given those, and the langrange multiplier mu (a scalar), return\n",
    "    the value of the merrit function\n",
    "    \"\"\"\n",
    "    cost = eval_collocation(env, x)\n",
    "    cons = constraints(env, x)\n",
    "    \n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return merit_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalty Formulation\n",
    "First, we will implement the penalty formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1.5\n",
    "mu = 1\n",
    "init_states_and_actions = np.random.uniform(low=-.1, high=.1, size=(env.H * (env.du + env.dx),))\n",
    "num_iter = 5\n",
    "\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \"\"\"\n",
    "    Otimization of the penalty function, which after finding the minimium for the merrit function we increase the\n",
    "    value of mu.\n",
    "    \"\"\"  \n",
    "    \n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    \n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    \n",
    "    \n",
    "    res = minimize(lambda x: merit_function(env, mu, x),\n",
    "               init_states_and_actions,\n",
    "               method='BFGS', \n",
    "               options={'xtol': 1e-6, 'disp': False, 'verbose': 2, 'maxiter':201}\n",
    "              )\n",
    "    print(\"\\nIteration %d:\"% i)\n",
    "    print(\"Value of mu %.3f\" % mu)\n",
    "    print(\"Inner optimization: %s\" % res.message)\n",
    "    print(\"Value of merit function %.3f\" % res.fun)\n",
    "    if np.linalg.norm(init_states_and_actions - res.x) < 1e-6: break\n",
    "    init_states_and_actions = res.x\n",
    "    \n",
    "    \n",
    "\n",
    "states_var_penalty, act_penalty = res.x[:env.H * env.dx], res.x[env.H * env.dx:]\n",
    "states_var_penalty = states_var_penalty.reshape(env.H, env.dx)\n",
    "act_penalty = ActPolicy(env, act_penalty) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_states_and_actions = np.random.uniform(low=-.1, high=.1, size=(env.H * (env.du + env.dx),))\n",
    "mu = 1.5\n",
    "alpha = 1e-3\n",
    "num_iter = 5\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \"\"\"\n",
    "    Otimization using dual descent, at each iteration we find the optimal for the merrit function, and then take\n",
    "    a gradient step for mu.\n",
    "    \"\"\" \n",
    "    res = minimize(lambda x: merit_function(env, mu, x),\n",
    "               init_states_and_actions,\n",
    "               method='BFGS', \n",
    "               options={'xtol': 1e-6, 'disp': False, 'verbose': 0, 'maxiter':201}\n",
    "              )\n",
    "    print(\"\\nIteration %d:\"% i)\n",
    "    print(\"Value of mu %.3f\" % mu)\n",
    "    print(\"Inner optimization: %s\" % res.message)\n",
    "    print(\"Value of merit function %.3f\" % res.fun)\n",
    "    if np.linalg.norm(init_states_and_actions - res.x) < 1e-6: break\n",
    "        \n",
    "    init_states_and_actions = res.x\n",
    "    \n",
    "    \"\"\"\n",
    "    Use the function constraints(env, init_state_and_actions) and the learning rate alpha to update the\n",
    "    value of mu.\n",
    "    \"\"\"\n",
    "    \"\"\"YOUR CODE HERE \"\"\"\n",
    "    \n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    \n",
    "\n",
    "states_var_dual_descent, act_dual_descent = res.x[:env.H * env.dx], res.x[env.H * env.dx:]\n",
    "states_var_dual_descent = states_var_dual_descent.reshape(env.H, env.dx)\n",
    "act_dual_descent = ActPolicy(env, act_dual_descent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_penalty, states_penalty = rollout(env, act_penalty)\n",
    "cost_dual_descent, states_dual_descent = rollout(env, act_dual_descent)\n",
    "states_penalty, states_dual_descent = np.array(states_penalty), np.array(states_dual_descent)\n",
    "error_penalty = np.linalg.norm(states_penalty[1:,:] - np.array(states_var_penalty))\n",
    "error_dual_descent = np.linalg.norm(states_dual_descent[1:,:] - np.array(states_var_dual_descent))\n",
    "\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Cost Penalty %.3f\" % cost_penalty)\n",
    "print(\"Cost Dual Descent %.3f\" % cost_dual_descent)\n",
    "\n",
    "print(\"Error Penalty %.3f\" % error_penalty)\n",
    "print(\"Error Dual Descent %.3f\" % error_dual_descent)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of the angle and angular velocity of the cart-pole environment across 20 timesteps for the penalty and dual descent methods.\")\n",
    "print(\"Penalty: solid line(-);  Dual descent: dashed line(--).\")\n",
    "ts = np.arange(states_penalty.shape[0])\n",
    "plt.plot(ts, states_penalty[:, 0], '-', ts, states_dual_descent[:, 0], '--')\n",
    "plt.plot(ts, states_penalty[:, 2], '-', ts, states_dual_descent[:, 2], '--')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
